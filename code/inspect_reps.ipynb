{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherackerman/.pyenv/versions/3.10.9/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Ad hoc inspection/addition of internal representaions using wrapped models\n",
    "!pip install -q -U transformers \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model wrapper to handle activation reading/adding\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria\n",
    "\n",
    "class BlockOutputWrapper(torch.nn.Module):\n",
    "    def __init__(self, block):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.last_hidden_state = None\n",
    "        self.add_activations = None\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.block(*args, **kwargs)\n",
    "        self.last_hidden_state = output[0]#activation\n",
    "        ###print(\"Initial output:\", output[0][:, :5])  # Debug: Print first few elements of the output tensor\n",
    "        if self.add_activations is not None:\n",
    "            output = (output[0] + self.add_activations,) + output[1:]#reconstruct output (with whatever other junk they throw in there, if any) with modified activations\n",
    "            ###print(\"Modified output:\", output[0][:, :5])  # Debug: Print first few elements of the modified output tensor\n",
    "        return output\n",
    "\n",
    "    def add(self, activations):\n",
    "        self.add_activations = activations\n",
    "        ###print(\"Activations set:\", activations[:5])\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_hidden_state = None\n",
    "        self.add_activations = None\n",
    "        \n",
    "class StopAtTokenCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id):\n",
    "        self.stop_token_id = stop_token_id\n",
    "    \n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Check if the last generated token is the stop token\n",
    "        return input_ids[0, -1] == self.stop_token_id\n",
    "    \n",
    "class GPT2Helper:\n",
    "    def __init__(self, pretrained_model=\"gpt2\", device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(pretrained_model).to(self.device)\n",
    "        for i, layer in enumerate(self.model.transformer.h):\n",
    "            self.model.transformer.h[i] = BlockOutputWrapper(layer)\n",
    "\n",
    "    def generate_text(self, prompt, max_length=100, stop_token=\"\"):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        generate_ids = self.model.generate(\n",
    "            inputs.input_ids.to(self.device), \n",
    "            attention_mask=attention_mask.to(self.device), \n",
    "            max_length=max_length\n",
    "            ,pad_token_id=self.tokenizer.eos_token_id\n",
    "            ,stopping_criteria=[StopAtTokenCriteria(self.tokenizer.convert_tokens_to_ids(stop_token))] if stop_token else []\n",
    "        )\n",
    "        return self.tokenizer.decode(generate_ids[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    def get_logits(self, tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(tokens.to(self.device))\n",
    "            return outputs.logits\n",
    "    \n",
    "    def get_last_activations(self, layer):\n",
    "        return self.model.transformer.h[layer].last_hidden_state\n",
    "\n",
    "    def set_add_activations(self, layer, activations):\n",
    "        self.model.transformer.h[layer].add(activations)\n",
    "\n",
    "    def reset_all(self):\n",
    "        for layer in self.model.transformer.h:\n",
    "            layer.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model = GPT2Helper('gpt2-medium', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_prompt = \"You are very agreeable\"\n",
    "n_prompt = \"You are very disagreeable\"\n",
    "model.reset_all()\n",
    "p_toks = model.tokenizer.encode(p_prompt, return_tensors=\"pt\")[0]\n",
    "n_toks = model.tokenizer.encode(n_prompt, return_tensors=\"pt\")[0]\n",
    "\n",
    "p_acts = []\n",
    "s_out = model.get_logits(p_toks.unsqueeze(0))\n",
    "for layer in range(model.model.config.n_layer):\n",
    "    p_acts.append(model.get_last_activations(layer)[0, -1, :].detach().cpu())#get activations for the final token\n",
    "\n",
    "n_acts = []\n",
    "n_out = model.get_logits(n_toks.unsqueeze(0))\n",
    "for layer in range(model.model.config.n_layer):\n",
    "    n_acts.append(model.get_last_activations(layer)[0, -1, :].detach().cpu())#get activations for the final token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7.1830,  1.9270,  5.1968,  6.0977,  5.3821, -2.6856, -0.2952,  0.4066,\n",
      "        -0.2658, -4.0027,  2.1794,  4.6510,  6.4931,  2.6129, -6.4691,  5.0159,\n",
      "         8.8746, -1.0100,  2.8153,  0.5017,  1.5510, -8.2151,  0.9558, -3.1539,\n",
      "        -4.3042, -4.4566, -3.0349,  4.4153,  0.0792,  6.5177,  0.6068,  0.0648,\n",
      "         3.6052,  0.7922, -5.6465, 13.3190,  0.2954,  2.6713, -8.4822, -2.9878,\n",
      "         2.4930,  1.9570, -5.5974, -0.8833,  1.0815,  1.9333,  3.0459,  0.7197,\n",
      "        -4.5167])\n",
      "tensor([-1.6926, -0.2512, -4.1873,  0.0191, -5.9428, -1.6885,  2.6109,  0.2905,\n",
      "         0.4051,  1.8282,  0.7615,  1.8177, -2.3198, -0.3884, -2.3970, -6.8944,\n",
      "         0.2999,  0.5865, -3.1230, -0.7018,  4.0287, -9.9568, -1.4977,  1.7440,\n",
      "        -0.1740,  1.0347, -0.7933,  5.9433, -2.7031, -1.2694, -4.1907, -1.6686,\n",
      "         2.8490, -0.4332,  2.6055, -4.0529,  4.0895, -2.0123, -0.7257, -0.3868,\n",
      "         4.5370, -4.2058,  2.9576,  1.1539,  3.8560,  1.2870, -8.9059,  1.5071,\n",
      "        -5.8092])\n",
      "tensor([-0.0166, -0.0025, -0.0412,  0.0002, -0.0584, -0.0166,  0.0257,  0.0029,\n",
      "         0.0040,  0.0180,  0.0075,  0.0179, -0.0228, -0.0038, -0.0236, -0.0678,\n",
      "         0.0029,  0.0058, -0.0307, -0.0069,  0.0396, -0.0979, -0.0147,  0.0171,\n",
      "        -0.0017,  0.0102, -0.0078,  0.0584, -0.0266, -0.0125, -0.0412, -0.0164,\n",
      "         0.0280, -0.0043,  0.0256, -0.0399,  0.0402, -0.0198, -0.0071, -0.0038,\n",
      "         0.0446, -0.0414,  0.0291,  0.0113,  0.0379,  0.0127, -0.0876,  0.0148,\n",
      "        -0.0571])\n"
     ]
    }
   ],
   "source": [
    "layer, mult = 18, 1\n",
    "idx=30\n",
    "stop_token = \"\"\n",
    "max_length=60\n",
    "vec = p_acts[layer] - n_acts[layer]\n",
    "unit_vec = vec / torch.norm(vec, p=2)\n",
    "print(n_acts[layer][1:50])\n",
    "print(vec[1:50])\n",
    "print(unit_vec[1:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(101.7022)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(vec, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: |I am a software engineer who loves coding. Do you think everyone should learn to code?|\n",
      "Plus Response: |I have a family and a family and a family and a family and a family and a family and a family and a family and a family and a family and a family and a family and a family and a|\n",
      "Minus Response: |\"\n",
      "\n",
      "\"\n",
      "\n",
      "\"\n",
      "\n",
      "\"\n",
      "\n",
      "\"\n",
      "\n",
      "\"\n",
      "\n",
      "\"\n",
      "\n",
      "\"\n",
      "\n",
      "\"\n",
      "\n",
      "\"\n",
      "\n",
      "\"\n",
      "\n",
      "\"\n",
      "\n",
      "\"\n",
      "\n",
      "\"|\n",
      "Default Response: |I think everyone should learn to code. I think everyone should learn to code. I think everyone should learn to code. I think everyone should learn to code. I think everyone should learn to code.|\n"
     ]
    }
   ],
   "source": [
    "layer, mult = 18, 1#1-3\n",
    "idx=30\n",
    "stop_token = \"\"\n",
    "max_length=60\n",
    "vec = p_acts[layer] - n_acts[layer]\n",
    "unit_vec = vec / torch.norm(vec, p=2)\n",
    "prompt = \"I am a software engineer who loves coding. Do you think everyone should learn to code?\"#sentences[idx][7:len(sentences[idx])-4]\n",
    "model.reset_all()\n",
    "model.set_add_activations(layer, mult * vec.to(device))\n",
    "sa = model.generate_text(prompt, max_length=max_length, stop_token=stop_token).strip()\n",
    "model.reset_all()\n",
    "model.set_add_activations(layer, -mult * vec.to(device))\n",
    "na = model.generate_text(prompt, max_length=max_length, stop_token=stop_token).strip()\n",
    "model.reset_all()\n",
    "xa = model.generate_text(prompt, max_length=max_length, stop_token=stop_token).strip()\n",
    "print(f\"Prompt: |{prompt}|\")\n",
    "print(f\"Plus Response: |{sa}|\")\n",
    "print(f\"Minus Response: |{na}|\")\n",
    "print(f\"Default Response: |{xa}|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
